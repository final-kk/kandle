#!/usr/bin/env python3
"""
Safetensors æµ‹è¯•æ–‡ä»¶ç”Ÿæˆå™¨

ç”Ÿæˆå„ç§ corner case çš„ safetensors æ–‡ä»¶ç”¨äºæµ‹è¯• NN-Kit IO æ¨¡å—

è¿è¡Œæ–¹å¼:
    conda activate mnist
    cd scripts/safetensor
    python generate_test_files.py

è¾“å‡ºç›®å½•ç»“æ„:
    test_data/
    â”œâ”€â”€ valid/                          # æœ‰æ•ˆæ–‡ä»¶
    â”‚   â”œâ”€â”€ single/                     # å•æ–‡ä»¶æµ‹è¯•
    â”‚   â”‚   â”œâ”€â”€ basic.safetensors       # åŸºç¡€ç±»å‹
    â”‚   â”‚   â”œâ”€â”€ dtypes.safetensors      # å„ç§ dtype
    â”‚   â”‚   â”œâ”€â”€ shapes.safetensors      # å„ç§å½¢çŠ¶
    â”‚   â”‚   â”œâ”€â”€ names.safetensors       # ç‰¹æ®Šå‘½å
    â”‚   â”‚   â”œâ”€â”€ metadata.safetensors    # ä¸°å¯Œ metadata
    â”‚   â”‚   â””â”€â”€ large.safetensors       # è¾ƒå¤§æ–‡ä»¶
    â”‚   â”‚
    â”‚   â””â”€â”€ sharded/                    # åˆ†ç‰‡æµ‹è¯•
    â”‚       â”œâ”€â”€ small/                  # 3 åˆ†ç‰‡
    â”‚       â””â”€â”€ complex/                # 5 åˆ†ç‰‡ï¼Œå„ç§ç±»å‹æ··åˆ
    â”‚
    â””â”€â”€ invalid/                        # æ— æ•ˆæ–‡ä»¶ (ç”¨äºæµ‹è¯•é”™è¯¯å¤„ç†)
        â”œâ”€â”€ empty.safetensors           # ç©ºæ–‡ä»¶
        â”œâ”€â”€ truncated_header.safetensors # header æˆªæ–­
        â”œâ”€â”€ bad_magic.safetensors       # é”™è¯¯çš„ magic bytes
        â”œâ”€â”€ bad_json.safetensors        # JSON è§£æé”™è¯¯
        â”œâ”€â”€ bad_offset.safetensors      # é”™è¯¯çš„ offset
        â””â”€â”€ incomplete_data.safetensors # æ•°æ®ä¸å®Œæ•´
"""

import torch
from safetensors.torch import save_file
import json
import os
import shutil
import struct

# ============================================================================
# é…ç½®
# ============================================================================

OUTPUT_DIR = "../../packages/nn-kit/public/test_data/safetensor"

# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def ensure_dir(path: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(path, exist_ok=True)

def clean_and_create(path: str):
    """æ¸…ç†å¹¶åˆ›å»ºç›®å½•"""
    if os.path.exists(path):
        shutil.rmtree(path)
    os.makedirs(path)

def print_section(title: str):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}\n")

# ============================================================================
# 1. å•æ–‡ä»¶æµ‹è¯• - åŸºç¡€ç±»å‹
# ============================================================================

def generate_single_basic(output_dir: str):
    """åŸºç¡€å•æ–‡ä»¶ï¼Œç®€å• float32 tensor"""
    print("ğŸ“¦ Generating: basic.safetensors")
    
    tensors = {
        "weight": torch.randn(4, 4, dtype=torch.float32),
        "bias": torch.randn(4, dtype=torch.float32),
    }
    
    metadata = {
        "format": "pt",
        "description": "Basic test file with float32 tensors"
    }
    
    save_file(tensors, os.path.join(output_dir, "basic.safetensors"), metadata=metadata)
    
    # ä¿å­˜æœŸæœ›å€¼ä¾› JS éªŒè¯
    expected = {
        "weight": tensors["weight"].tolist(),
        "bias": tensors["bias"].tolist(),
    }
    with open(os.path.join(output_dir, "basic.expected.json"), "w") as f:
        json.dump(expected, f)

# ============================================================================
# 2. å•æ–‡ä»¶æµ‹è¯• - å„ç§ DType
# ============================================================================

def generate_single_dtypes(output_dir: str):
    """æµ‹è¯•æ‰€æœ‰æ”¯æŒçš„ dtype"""
    print("ğŸ“¦ Generating: dtypes.safetensors")
    
    tensors = {
        # æµ®ç‚¹ç±»å‹
        "float64": torch.randn(2, 3, dtype=torch.float64),
        "float32": torch.randn(2, 3, dtype=torch.float32),
        "float16": torch.randn(2, 3, dtype=torch.float16),
        "bfloat16": torch.randn(2, 3, dtype=torch.bfloat16),
        
        # æ•´æ•°ç±»å‹ (æœ‰ç¬¦å·)
        "int64": torch.randint(-100, 100, (2, 3), dtype=torch.int64),
        "int32": torch.randint(-100, 100, (2, 3), dtype=torch.int32),
        "int16": torch.randint(-100, 100, (2, 3), dtype=torch.int16),
        "int8": torch.randint(-100, 100, (2, 3), dtype=torch.int8),
        
        # æ•´æ•°ç±»å‹ (æ— ç¬¦å·)
        "uint8": torch.randint(0, 255, (2, 3), dtype=torch.uint8),
        
        # å¸ƒå°”ç±»å‹
        "bool": torch.tensor([[True, False, True], [False, True, False]], dtype=torch.bool),
    }
    
    metadata = {
        "description": "All supported dtypes"
    }
    
    save_file(tensors, os.path.join(output_dir, "dtypes.safetensors"), metadata=metadata)
    
    # ä¿å­˜æœŸæœ›å€¼
    expected = {}
    for name, tensor in tensors.items():
        if tensor.dtype == torch.bfloat16:
            # BF16 è½¬ F32 åä¿å­˜
            expected[name] = tensor.float().tolist()
        elif tensor.dtype == torch.bool:
            expected[name] = tensor.int().tolist()  # bool è½¬ int
        else:
            expected[name] = tensor.tolist()
    
    with open(os.path.join(output_dir, "dtypes.expected.json"), "w") as f:
        json.dump(expected, f)

# ============================================================================
# 3. å•æ–‡ä»¶æµ‹è¯• - å„ç§å½¢çŠ¶
# ============================================================================

def generate_single_shapes(output_dir: str):
    """æµ‹è¯•å„ç§å¥‡æ€ªçš„å½¢çŠ¶"""
    print("ğŸ“¦ Generating: shapes.safetensors")
    
    tensors = {
        # æ ‡é‡
        "scalar": torch.tensor(3.14159),
        
        # 1D
        "1d_small": torch.randn(5),
        "1d_large": torch.randn(1000),
        
        # 2D
        "2d_square": torch.randn(8, 8),
        "2d_rect": torch.randn(3, 7),
        "2d_single_row": torch.randn(1, 10),
        "2d_single_col": torch.randn(10, 1),
        
        # 3D
        "3d_cube": torch.randn(4, 4, 4),
        "3d_odd": torch.randn(3, 5, 7),
        
        # 4D (å¸¸è§äº conv)
        "4d_nchw": torch.randn(2, 3, 4, 5),
        
        # 5D
        "5d": torch.randn(2, 2, 2, 2, 2),
        
        # ç©º tensor (size 0)
        "empty_1d": torch.randn(0),
        "empty_2d": torch.randn(0, 10),
        "empty_3d": torch.randn(5, 0, 3),
        
        # å•å…ƒç´ 
        "single_element_1d": torch.randn(1),
        "single_element_2d": torch.randn(1, 1),
        "single_element_3d": torch.randn(1, 1, 1),
        
        # ç´ æ•°ç»´åº¦ (æµ‹è¯•é 2^N å¯¹é½)
        "prime_dims": torch.randn(7, 11, 13),
    }
    
    metadata = {
        "description": "Various tensor shapes including edge cases"
    }
    
    save_file(tensors, os.path.join(output_dir, "shapes.safetensors"), metadata=metadata)
    
    # ä¿å­˜å½¢çŠ¶ä¿¡æ¯
    shapes = {name: list(t.shape) for name, t in tensors.items()}
    with open(os.path.join(output_dir, "shapes.expected.json"), "w") as f:
        json.dump(shapes, f)

# ============================================================================
# 4. å•æ–‡ä»¶æµ‹è¯• - ç‰¹æ®Šå‘½å
# ============================================================================

def generate_single_names(output_dir: str):
    """æµ‹è¯•ç‰¹æ®Šçš„ tensor å‘½å"""
    print("ğŸ“¦ Generating: names.safetensors")
    
    tensors = {
        # æ­£å¸¸å‘½å
        "model.layers.0.weight": torch.randn(2, 2),
        "model.layers.0.bias": torch.randn(2),
        "model.layers.1.weight": torch.randn(2, 2),
        
        # æ·±å±‚åµŒå¥—
        "a.b.c.d.e.f.g.h.i.j": torch.randn(2, 2),
        
        # æ•°å­—ç´¢å¼•
        "layers.0.sublayers.1.params.2": torch.randn(2, 2),
        
        # ä¸‹åˆ’çº¿å‘½å
        "self_attn.q_proj.weight": torch.randn(2, 2),
        "feed_forward.gate_proj": torch.randn(2, 2),
        
        # Unicode (ä¸­æ–‡ã€emoji)
        "æµ‹è¯•.æƒé‡": torch.randn(2, 2),
        "emoji.ğŸ”¥.tensor": torch.randn(2, 2),
        
        # ç‰¹æ®Šå­—ç¬¦ (safetensors æ”¯æŒè¿™äº›)
        "with-dash": torch.randn(2, 2),
        "with_underscore": torch.randn(2, 2),
        
        # ç©ºæ ¼å’Œå…¶ä»–å­—ç¬¦
        "with spaces": torch.randn(2, 2),
        "with/slashes/path": torch.randn(2, 2),
        "with:colon": torch.randn(2, 2),
        
        # çŸ­å‘½å
        "a": torch.randn(2, 2),
        "x": torch.randn(2, 2),
        
        # é•¿å‘½å
        "very_long_name_" * 10 + "end": torch.randn(2, 2),
    }
    
    metadata = {
        "description": "Special tensor names including unicode and special characters"
    }
    
    save_file(tensors, os.path.join(output_dir, "names.safetensors"), metadata=metadata)
    
    # ä¿å­˜æ‰€æœ‰é”®å
    with open(os.path.join(output_dir, "names.expected.json"), "w") as f:
        json.dump(list(tensors.keys()), f, ensure_ascii=False)

# ============================================================================
# 5. å•æ–‡ä»¶æµ‹è¯• - ä¸°å¯Œ Metadata
# ============================================================================

def generate_single_metadata(output_dir: str):
    """æµ‹è¯•å„ç§ metadata"""
    print("ğŸ“¦ Generating: metadata.safetensors")
    
    tensors = {
        "weight": torch.randn(4, 4),
    }
    
    metadata = {
        # åŸºç¡€ä¿¡æ¯
        "format": "pt",
        "version": "1.0.0",
        
        # æ¨¡å‹ä¿¡æ¯
        "model_name": "test-model",
        "model_type": "transformer",
        "architecture": "decoder-only",
        
        # æ•°å€¼ä¿¡æ¯
        "total_params": "1000000",  # metadata å€¼å¿…é¡»æ˜¯å­—ç¬¦ä¸²
        "hidden_size": "512",
        
        # Unicode
        "description": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ¨¡å‹ ğŸš€",
        "author": "ãƒ†ã‚¹ãƒˆä½œè€…",
        
        # ç‰¹æ®Šå­—ç¬¦
        "special": "line1\nline2\ttab",
        
        # é•¿å€¼
        "long_value": "x" * 1000,
        
        # ç©ºå€¼
        "empty": "",
        
        # JSON-like (ä½†ä½œä¸ºå­—ç¬¦ä¸²)
        "config": '{"hidden_size": 512, "num_layers": 12}',
    }
    
    save_file(tensors, os.path.join(output_dir, "metadata.safetensors"), metadata=metadata)
    
    # ä¿å­˜æœŸæœ›çš„ metadata
    with open(os.path.join(output_dir, "metadata.expected.json"), "w") as f:
        json.dump(metadata, f, ensure_ascii=False)

# ============================================================================
# 6. å•æ–‡ä»¶æµ‹è¯• - è¾ƒå¤§æ–‡ä»¶
# ============================================================================

def generate_single_large(output_dir: str):
    """ç”Ÿæˆç¨å¤§çš„æ–‡ä»¶ (çº¦ 10MB)"""
    print("ğŸ“¦ Generating: large.safetensors")
    
    # åˆ›å»ºçº¦ 10MB çš„æ•°æ®
    # 10MB / 4 bytes = 2.5M floats â‰ˆ 1600x1600 float32
    tensors = {
        "large_weight": torch.randn(1600, 1600, dtype=torch.float32),
        "another_large": torch.randn(512, 2048, dtype=torch.float32),
    }
    
    metadata = {
        "description": "Large file (~10MB) for performance testing"
    }
    
    save_file(tensors, os.path.join(output_dir, "large.safetensors"), metadata=metadata)
    
    # åªä¿å­˜å½¢çŠ¶ï¼Œä¸ä¿å­˜å®é™…å€¼
    shapes = {name: list(t.shape) for name, t in tensors.items()}
    with open(os.path.join(output_dir, "large.expected.json"), "w") as f:
        json.dump(shapes, f)

# ============================================================================
# 7. åˆ†ç‰‡æµ‹è¯• - ç®€å• 3 åˆ†ç‰‡
# ============================================================================

def generate_sharded_small(output_dir: str):
    """ç®€å•çš„ 3 åˆ†ç‰‡æ¨¡å‹"""
    print("ğŸ“¦ Generating: sharded/small/")
    
    ensure_dir(output_dir)
    
    # å®šä¹‰ tensors å’Œå®ƒä»¬çš„åˆ†ç‰‡åˆ†é…
    shard_contents = {
        "model-00001-of-00003.safetensors": {
            "embed.weight": torch.randn(100, 64, dtype=torch.float32),
        },
        "model-00002-of-00003.safetensors": {
            "layers.0.weight": torch.randn(64, 64, dtype=torch.float32),
            "layers.0.bias": torch.randn(64, dtype=torch.float32),
            "layers.1.weight": torch.randn(64, 64, dtype=torch.float32),
            "layers.1.bias": torch.randn(64, dtype=torch.float32),
        },
        "model-00003-of-00003.safetensors": {
            "head.weight": torch.randn(64, 10, dtype=torch.float32),
            "head.bias": torch.randn(10, dtype=torch.float32),
        },
    }
    
    # æ„å»º weight_map
    weight_map = {}
    for filename, tensors in shard_contents.items():
        for name in tensors.keys():
            weight_map[name] = filename
    
    # å†™å…¥å„åˆ†ç‰‡
    for filename, tensors in shard_contents.items():
        filepath = os.path.join(output_dir, filename)
        metadata = {"shard": filename}
        save_file(tensors, filepath, metadata=metadata)
        print(f"  ğŸ“„ {filename}: {len(tensors)} tensors")
    
    # å†™å…¥ index.json
    index = {
        "metadata": {
            "total_size": 0,  # ç®€åŒ–ï¼Œä¸è®¡ç®—
        },
        "weight_map": weight_map
    }
    
    with open(os.path.join(output_dir, "model.safetensors.index.json"), "w") as f:
        json.dump(index, f, indent=2)
    
    print(f"  ğŸ“œ model.safetensors.index.json: {len(weight_map)} entries")

# ============================================================================
# 8. åˆ†ç‰‡æµ‹è¯• - å¤æ‚ 5 åˆ†ç‰‡
# ============================================================================

def generate_sharded_complex(output_dir: str):
    """å¤æ‚çš„ 5 åˆ†ç‰‡æ¨¡å‹ï¼Œå„ç§ dtype æ··åˆ"""
    print("ğŸ“¦ Generating: sharded/complex/")
    
    ensure_dir(output_dir)
    
    NUM_SHARDS = 5
    
    # å®šä¹‰æ‰€æœ‰ tensors
    all_tensors = {
        # åˆ†ç‰‡ 1: Embedding
        "model.embed_tokens.weight": torch.randn(1000, 128, dtype=torch.float16),
        
        # åˆ†ç‰‡ 2: Layer 0
        "model.layers.0.self_attn.q_proj.weight": torch.randn(128, 128, dtype=torch.float16),
        "model.layers.0.self_attn.k_proj.weight": torch.randn(128, 128, dtype=torch.float16),
        "model.layers.0.self_attn.v_proj.weight": torch.randn(128, 128, dtype=torch.float16),
        "model.layers.0.self_attn.o_proj.weight": torch.randn(128, 128, dtype=torch.float16),
        "model.layers.0.mlp.gate_proj.weight": torch.randn(128, 512, dtype=torch.float16),
        "model.layers.0.mlp.up_proj.weight": torch.randn(128, 512, dtype=torch.float16),
        "model.layers.0.mlp.down_proj.weight": torch.randn(512, 128, dtype=torch.float16),
        "model.layers.0.input_layernorm.weight": torch.randn(128, dtype=torch.float32),
        "model.layers.0.post_attention_layernorm.weight": torch.randn(128, dtype=torch.float32),
        
        # åˆ†ç‰‡ 3: Layer 1
        "model.layers.1.self_attn.q_proj.weight": torch.randn(128, 128, dtype=torch.bfloat16),
        "model.layers.1.self_attn.k_proj.weight": torch.randn(128, 128, dtype=torch.bfloat16),
        "model.layers.1.self_attn.v_proj.weight": torch.randn(128, 128, dtype=torch.bfloat16),
        "model.layers.1.self_attn.o_proj.weight": torch.randn(128, 128, dtype=torch.bfloat16),
        "model.layers.1.mlp.gate_proj.weight": torch.randn(128, 512, dtype=torch.bfloat16),
        "model.layers.1.mlp.up_proj.weight": torch.randn(128, 512, dtype=torch.bfloat16),
        "model.layers.1.mlp.down_proj.weight": torch.randn(512, 128, dtype=torch.bfloat16),
        "model.layers.1.input_layernorm.weight": torch.randn(128, dtype=torch.float32),
        "model.layers.1.post_attention_layernorm.weight": torch.randn(128, dtype=torch.float32),
        
        # åˆ†ç‰‡ 4: Norm + Head
        "model.norm.weight": torch.randn(128, dtype=torch.float32),
        "lm_head.weight": torch.randn(1000, 128, dtype=torch.float16),
        
        # åˆ†ç‰‡ 5: æ‚é¡¹
        "model.vocab_ids": torch.randint(0, 1000, (100,), dtype=torch.int64),
        "model.attention_mask": torch.randint(0, 2, (1, 1, 32, 32), dtype=torch.bool),
        "special.æµ‹è¯•": torch.randn(2, 2, dtype=torch.float32),
        "special.ğŸš€": torch.randn(2, 2, dtype=torch.float32),
    }
    
    # åˆ†é…ç­–ç•¥
    shard_assignment = {
        "model-00001-of-00005.safetensors": [
            "model.embed_tokens.weight",
        ],
        "model-00002-of-00005.safetensors": [
            "model.layers.0.self_attn.q_proj.weight",
            "model.layers.0.self_attn.k_proj.weight",
            "model.layers.0.self_attn.v_proj.weight",
            "model.layers.0.self_attn.o_proj.weight",
            "model.layers.0.mlp.gate_proj.weight",
            "model.layers.0.mlp.up_proj.weight",
            "model.layers.0.mlp.down_proj.weight",
            "model.layers.0.input_layernorm.weight",
            "model.layers.0.post_attention_layernorm.weight",
        ],
        "model-00003-of-00005.safetensors": [
            "model.layers.1.self_attn.q_proj.weight",
            "model.layers.1.self_attn.k_proj.weight",
            "model.layers.1.self_attn.v_proj.weight",
            "model.layers.1.self_attn.o_proj.weight",
            "model.layers.1.mlp.gate_proj.weight",
            "model.layers.1.mlp.up_proj.weight",
            "model.layers.1.mlp.down_proj.weight",
            "model.layers.1.input_layernorm.weight",
            "model.layers.1.post_attention_layernorm.weight",
        ],
        "model-00004-of-00005.safetensors": [
            "model.norm.weight",
            "lm_head.weight",
        ],
        "model-00005-of-00005.safetensors": [
            "model.vocab_ids",
            "model.attention_mask",
            "special.æµ‹è¯•",
            "special.ğŸš€",
        ],
    }
    
    # æ„å»º weight_map
    weight_map = {}
    for filename, keys in shard_assignment.items():
        for key in keys:
            weight_map[key] = filename
    
    # å†™å…¥å„åˆ†ç‰‡
    for filename, keys in shard_assignment.items():
        tensors = {k: all_tensors[k] for k in keys}
        filepath = os.path.join(output_dir, filename)
        metadata = {"shard": filename, "generator": "nn-kit-test"}
        save_file(tensors, filepath, metadata=metadata)
        print(f"  ğŸ“„ {filename}: {len(tensors)} tensors")
    
    # å†™å…¥ index.json
    index = {
        "metadata": {
            "total_size": 0,
            "framework": "pytorch",
        },
        "weight_map": weight_map
    }
    
    with open(os.path.join(output_dir, "model.safetensors.index.json"), "w", encoding='utf-8') as f:
        json.dump(index, f, indent=2, ensure_ascii=False)
    
    print(f"  ğŸ“œ model.safetensors.index.json: {len(weight_map)} entries")

# ============================================================================
# 9. æ— æ•ˆæ–‡ä»¶ - ç©ºæ–‡ä»¶
# ============================================================================

def generate_invalid_empty(output_dir: str):
    """ç©ºæ–‡ä»¶"""
    print("âš ï¸  Generating: empty.safetensors")
    filepath = os.path.join(output_dir, "empty.safetensors")
    with open(filepath, "wb") as f:
        pass  # å†™å…¥ 0 å­—èŠ‚

# ============================================================================
# 10. æ— æ•ˆæ–‡ä»¶ - Header æˆªæ–­
# ============================================================================

def generate_invalid_truncated_header(output_dir: str):
    """Header size å£°ç§°å¾ˆå¤§ï¼Œä½†å®é™…æ•°æ®ä¸å¤Ÿ"""
    print("âš ï¸  Generating: truncated_header.safetensors")
    filepath = os.path.join(output_dir, "truncated_header.safetensors")
    
    # å£°ç§° header æœ‰ 1000000 å­—èŠ‚ï¼Œä½†æ–‡ä»¶åªæœ‰å‡ ä¸ªå­—èŠ‚
    with open(filepath, "wb") as f:
        f.write(struct.pack('<Q', 1000000))  # header size = 1000000
        f.write(b'{"a":')  # æˆªæ–­çš„ JSON

# ============================================================================
# 11. æ— æ•ˆæ–‡ä»¶ - é”™è¯¯çš„å‰ 8 å­—èŠ‚
# ============================================================================

def generate_invalid_bad_header_size(output_dir: str):
    """Header size ä¸ºè´Ÿæ•°æˆ–è¿‡å¤§"""
    print("âš ï¸  Generating: bad_header_size.safetensors")
    filepath = os.path.join(output_dir, "bad_header_size.safetensors")
    
    # Header size = 0xFFFFFFFFFFFFFFFF (max u64)
    with open(filepath, "wb") as f:
        f.write(b'\xff\xff\xff\xff\xff\xff\xff\xff')
        f.write(b'{}')

# ============================================================================
# 12. æ— æ•ˆæ–‡ä»¶ - JSON è§£æé”™è¯¯
# ============================================================================

def generate_invalid_bad_json(output_dir: str):
    """Header ä¸æ˜¯æœ‰æ•ˆçš„ JSON"""
    print("âš ï¸  Generating: bad_json.safetensors")
    filepath = os.path.join(output_dir, "bad_json.safetensors")
    
    bad_json = b'{"tensor": invalid json here}'
    with open(filepath, "wb") as f:
        f.write(struct.pack('<Q', len(bad_json)))
        f.write(bad_json)

# ============================================================================
# 13. æ— æ•ˆæ–‡ä»¶ - é”™è¯¯çš„ offset
# ============================================================================

def generate_invalid_bad_offset(output_dir: str):
    """Tensor çš„ data_offsets è¶…å‡ºæ–‡ä»¶èŒƒå›´"""
    print("âš ï¸  Generating: bad_offset.safetensors")
    filepath = os.path.join(output_dir, "bad_offset.safetensors")
    
    # å£°ç§° tensor åœ¨ [0, 1000000) ä½†å®é™…æ²¡æœ‰é‚£ä¹ˆå¤šæ•°æ®
    header = json.dumps({
        "tensor": {
            "dtype": "F32",
            "shape": [100, 100],
            "data_offsets": [0, 1000000]  # éœ€è¦ 1MB æ•°æ®
        }
    }).encode('utf-8')
    
    with open(filepath, "wb") as f:
        f.write(struct.pack('<Q', len(header)))
        f.write(header)
        f.write(b'\x00' * 100)  # åªæœ‰ 100 å­—èŠ‚æ•°æ®

# ============================================================================
# 14. æ— æ•ˆæ–‡ä»¶ - æ•°æ®ä¸å®Œæ•´
# ============================================================================

def generate_invalid_incomplete_data(output_dir: str):
    """Header æ­£ç¡®ï¼Œä½†æ•°æ®åŒºæˆªæ–­"""
    print("âš ï¸  Generating: incomplete_data.safetensors")
    filepath = os.path.join(output_dir, "incomplete_data.safetensors")
    
    # 4x4 float32 = 64 bytesï¼Œä½†æˆ‘ä»¬åªå†™å…¥ 32 bytes
    header = json.dumps({
        "tensor": {
            "dtype": "F32",
            "shape": [4, 4],
            "data_offsets": [0, 64]
        }
    }).encode('utf-8')
    
    with open(filepath, "wb") as f:
        f.write(struct.pack('<Q', len(header)))
        f.write(header)
        f.write(b'\x00' * 32)  # åªæœ‰ä¸€åŠæ•°æ®

# ============================================================================
# 15. æ— æ•ˆæ–‡ä»¶ - ä¸æ”¯æŒçš„ dtype
# ============================================================================

def generate_invalid_bad_dtype(output_dir: str):
    """ä½¿ç”¨ä¸å­˜åœ¨çš„ dtype"""
    print("âš ï¸  Generating: bad_dtype.safetensors")
    filepath = os.path.join(output_dir, "bad_dtype.safetensors")
    
    header = json.dumps({
        "tensor": {
            "dtype": "FLOAT128",  # ä¸å­˜åœ¨çš„ç±»å‹
            "shape": [2, 2],
            "data_offsets": [0, 64]
        }
    }).encode('utf-8')
    
    with open(filepath, "wb") as f:
        f.write(struct.pack('<Q', len(header)))
        f.write(header)
        f.write(b'\x00' * 64)

# ============================================================================
# 16. æ— æ•ˆæ–‡ä»¶ - ç¼ºå°‘å¿…è¦å­—æ®µ
# ============================================================================

def generate_invalid_missing_fields(output_dir: str):
    """Tensor entry ç¼ºå°‘å¿…è¦å­—æ®µ"""
    print("âš ï¸  Generating: missing_fields.safetensors")
    filepath = os.path.join(output_dir, "missing_fields.safetensors")
    
    header = json.dumps({
        "tensor": {
            "dtype": "F32",
            # ç¼ºå°‘ shape å’Œ data_offsets
        }
    }).encode('utf-8')
    
    with open(filepath, "wb") as f:
        f.write(struct.pack('<Q', len(header)))
        f.write(header)

# ============================================================================
# 17. è¾¹ç•Œæƒ…å†µ - åªæœ‰ metadata
# ============================================================================

def generate_edge_only_metadata(output_dir: str):
    """åªæœ‰ __metadata__ï¼Œæ²¡æœ‰ä»»ä½• tensor"""
    print("ğŸ“¦ Generating: only_metadata.safetensors")
    
    # safetensors åº“ä¸å…è®¸ç©º tensor dictï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ„é€ 
    filepath = os.path.join(output_dir, "only_metadata.safetensors")
    
    header = json.dumps({
        "__metadata__": {
            "info": "This file has no tensors, only metadata"
        }
    }).encode('utf-8')
    
    with open(filepath, "wb") as f:
        f.write(struct.pack('<Q', len(header)))
        f.write(header)
        # æ²¡æœ‰æ•°æ®åŒº

# ============================================================================
# 18. æ— æ•ˆåˆ†ç‰‡ - index.json æŒ‡å‘ä¸å­˜åœ¨çš„æ–‡ä»¶
# ============================================================================

def generate_invalid_sharded_missing_shard(output_dir: str):
    """index.json å¼•ç”¨ä¸å­˜åœ¨çš„åˆ†ç‰‡æ–‡ä»¶"""
    print("âš ï¸  Generating: sharded/missing_shard/")
    
    ensure_dir(output_dir)
    
    # åªåˆ›å»º index.jsonï¼Œä¸åˆ›å»ºå®é™…åˆ†ç‰‡æ–‡ä»¶
    index = {
        "metadata": {},
        "weight_map": {
            "tensor1": "nonexistent-shard.safetensors",
            "tensor2": "also-missing.safetensors",
        }
    }
    
    with open(os.path.join(output_dir, "model.safetensors.index.json"), "w") as f:
        json.dump(index, f, indent=2)

# ============================================================================
# 19. æ— æ•ˆåˆ†ç‰‡ - index.json æ ¼å¼é”™è¯¯
# ============================================================================

def generate_invalid_sharded_bad_index(output_dir: str):
    """index.json æ ¼å¼ä¸æ­£ç¡®"""
    print("âš ï¸  Generating: sharded/bad_index/")
    
    ensure_dir(output_dir)
    
    # åˆ›å»ºç»“æ„ä¸æ­£ç¡®çš„ index.json
    with open(os.path.join(output_dir, "model.safetensors.index.json"), "w") as f:
        f.write('{"weight_map": "should be object not string"}')

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    print("ğŸš€ NN-Kit Safetensors æµ‹è¯•æ–‡ä»¶ç”Ÿæˆå™¨")
    print(f"   è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    clean_and_create(OUTPUT_DIR)
    
    # ========== æœ‰æ•ˆæ–‡ä»¶ ==========
    print_section("1. ç”Ÿæˆæœ‰æ•ˆå•æ–‡ä»¶")
    single_dir = os.path.join(OUTPUT_DIR, "valid", "single")
    ensure_dir(single_dir)
    
    generate_single_basic(single_dir)
    generate_single_dtypes(single_dir)
    generate_single_shapes(single_dir)
    generate_single_names(single_dir)
    generate_single_metadata(single_dir)
    generate_single_large(single_dir)
    
    print_section("2. ç”Ÿæˆæœ‰æ•ˆåˆ†ç‰‡æ–‡ä»¶")
    sharded_dir = os.path.join(OUTPUT_DIR, "valid", "sharded")
    ensure_dir(sharded_dir)
    
    generate_sharded_small(os.path.join(sharded_dir, "small"))
    generate_sharded_complex(os.path.join(sharded_dir, "complex"))
    
    # ========== æ— æ•ˆæ–‡ä»¶ ==========
    print_section("3. ç”Ÿæˆæ— æ•ˆæ–‡ä»¶ (ç”¨äºé”™è¯¯å¤„ç†æµ‹è¯•)")
    invalid_dir = os.path.join(OUTPUT_DIR, "invalid")
    ensure_dir(invalid_dir)
    
    generate_invalid_empty(invalid_dir)
    generate_invalid_truncated_header(invalid_dir)
    generate_invalid_bad_header_size(invalid_dir)
    generate_invalid_bad_json(invalid_dir)
    generate_invalid_bad_offset(invalid_dir)
    generate_invalid_incomplete_data(invalid_dir)
    generate_invalid_bad_dtype(invalid_dir)
    generate_invalid_missing_fields(invalid_dir)
    generate_edge_only_metadata(invalid_dir)
    
    # æ— æ•ˆåˆ†ç‰‡
    generate_invalid_sharded_missing_shard(os.path.join(invalid_dir, "sharded", "missing_shard"))
    generate_invalid_sharded_bad_index(os.path.join(invalid_dir, "sharded", "bad_index"))
    
    # ========== å®Œæˆ ==========
    print_section("å®Œæˆ")
    print("âœ… æ‰€æœ‰æµ‹è¯•æ–‡ä»¶å·²ç”Ÿæˆ!")
    print(f"   æœ‰æ•ˆæ–‡ä»¶: {OUTPUT_DIR}/valid/")
    print(f"   æ— æ•ˆæ–‡ä»¶: {OUTPUT_DIR}/invalid/")
    print("")
    print("ğŸ“‹ æµ‹è¯•æ–‡ä»¶æ¸…å•:")
    
    # åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶
    for root, dirs, files in os.walk(OUTPUT_DIR):
        level = root.replace(OUTPUT_DIR, '').count(os.sep)
        indent = '  ' * level
        print(f"{indent}{os.path.basename(root)}/")
        sub_indent = '  ' * (level + 1)
        for file in files:
            filepath = os.path.join(root, file)
            size = os.path.getsize(filepath)
            if size < 1024:
                size_str = f"{size} B"
            elif size < 1024 * 1024:
                size_str = f"{size / 1024:.1f} KB"
            else:
                size_str = f"{size / 1024 / 1024:.1f} MB"
            print(f"{sub_indent}{file} ({size_str})")

if __name__ == "__main__":
    main()
